# 前のリポジトリと同じフォーマットで現在エラーが出る原因を特定するためのデバッグ用設定ファイル

name: qwen3_0.6b_continual_debug
dump_dir: output/qwen3_0.6b_continual_debug
seed: 777
grad_acc_steps: 1
gc_collect_freq: 100
probe_freq: null

steps: 100

data:
  root_dir: /home/pj24001974/ku50001532/lingua_self-distillation/data
  sources:
      fineweb_edu_10bt_shuffled: 1.0
  batch_size: 1 # per device batch size
  seq_len: 8192
  n_views: 2
  seed: 42
  add_bos: false
  add_eos: true
  load_async: true
  prefetch_size: 128
  tokenizer:
      name: hf # 矢野さんは hf を指定しているので，バグったら変更してみること
      path: model/qwen3-0.6b/tokenizer.json

optim:
  lr: 1e-5
  weight_decay: 0.1
  epsilon: 1e-8
  beta1: 0.9
  beta2: 0.95
  clip: 1.0
  scheduler: cosine
  warmup: 1000
  # decay_fraction: 0
  lr_min_ratio: 0.1
  cycle_length: 1.0

model:
  seed: 42
  dim: 2048
  ffn_dim_multiplier: 1.5
  n_layers: 16
  n_heads: 32
  n_kv_heads: 8
  weight_tying: true
  max_seqlen: 131072
  init_std_factor: global_depth
  rope_theta: 500000
  # rope_embeddings_config: "meta-llama/Llama-3.2-1B" # use pretrained config Llama 3.2
  norm_eps: 1e-5


distributed:
  fsdp_type: full_shard
  compile: true
  model_dtype: bf16
  matmul_allow_tf32: false
  selective_activation_checkpointing: false
  tp_size: 1

checkpoint:
  init_ckpt_path: model/llama-3.2-1b_dcp
  continue_training_from_init: false
  dump:
      every: 100
      keep: 1
  eval:
      every: 100
      keep: 1
  
# profiling:
#   run: true
#   mem_warmup: 0
#   mem_steps: 4
#   profile_warmup: 100
#   profile_steps: 4

logging:
  freq: 1
  acc_freq: null

  wandb:
    job_type: train
    dir: ./wandb
    project: SD_llama-1b_${model.dim}dim_${model.n_layers}_${model.n_heads}
    tags: 
      - self_distillation
      - debug
      - 1b
    resume: auto

async_eval_gpus: null

eval:
  harness: null
  validation:
    max_steps: 1000
  generator:
    max_tokens: 8192
    dtype: bf16

