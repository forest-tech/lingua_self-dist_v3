name: llama_3.2_1b_continual
dump_dir: output/llama_3.2_1b_continual
seed: 42
grad_acc_steps: 1
gc_collect_freq: 100
probe_freq: null

steps: 1000

data:
  root_dir: /home/pj24001974/ku50001532/lingua_self-distillation/data
  sources:
      fineweb_edu_10bt_shuffled: 1.0
  batch_size: 1 # per device batch size
  seq_len: 8192
  n_views: 2
  seed: 42
  add_bos: false
  add_eos: true
  load_async: true
  prefetch_size: 128
  tokenizer:
      name: tiktoken # 矢野さんは hf を指定しているので，バグったら変更してみること
      path: model/llama-3.2-1b/original/tokenizer.model

optim:
  lr: 1e-5
  weight_decay: 0.1
  epsilon: 1e-8
  beta1: 0.9
  beta2: 0.95
  clip: 1.0
  scheduler: cosine
  warmup: 1000
  # decay_fraction: 0
  lr_min_ratio: 0.1
  cycle_length: 1.0

model:
  seed: 42
  dim: 2048
  ffn_dim_multiplier: 1.5
  n_layers: 16
  n_heads: 32
  n_kv_heads: 8
  weight_tying: true
  max_seqlen: 131072
  init_std_factor: global_depth
  rope_theta: 500000
  # rope_embeddings_config: "meta-llama/Llama-3.2-1B" # use pretrained config Llama 3.2
  norm_eps: 1e-5

  
# distributed:
#   # dp_shard: 8 # In how many shard to split the model weight. Typically number gpu in a node.
#   # dp_replicate: 1 # How many times to replicate the model. Typically number of nodes.
#   tp_size: 1 # dp_replicate * dp_shard * tp_size == all number of gpus.
#   selective_activation_checkpointing: false
#   compile: false # Only true for tp_size == 1
#   fsdp_type: full_shard
#   model_dtype: bf16
#   matmul_allow_tf32: false
#   # allow_bf16_reduced_precision_reduction: false
#   # detect_anomaly: false
  
#   # compile_cache_size_limit: 8
#   # spawn_method: forkserver

distributed:
  fsdp_type: full_shard
  compile: true
  model_dtype: bf16
  matmul_allow_tf32: false
  selective_activation_checkpointing: false
  tp_size: 1

checkpoint:
  init_ckpt_path: model/llama-3.2-1b_dcp
  continue_training_from_init: false
  dump:
      every: 100
      keep: 1
  eval:
      every: 100
      keep: 1
  
# profiling:
#   run: true
#   mem_warmup: 0
#   mem_steps: 4
#   profile_warmup: 100
#   profile_steps: 4

logging:
  freq: 1
  acc_freq: null

  wandb:
    job_type: train
    dir: ./wandb
    project: SD_llama-1b_${model.dim}dim_${model.n_layers}_${model.n_heads}
    tags: 
      - self_distillation
      - debug
      - 1b
    resume: auto

async_eval_gpus: null

eval:
  harness: null
  validation:
    max_steps: 1000
  generator:
    max_tokens: 8192
    dtype: bf16

